{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch import optim\n",
    "from pathlib import Path\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader\n",
    "from asteroid import data\n",
    "\n",
    "from asteroid.models import ConvTasNet\n",
    "from asteroid.data import MUSDB18Dataset\n",
    "from asteroid.losses import PITLossWrapper\n",
    "from asteroid.losses import pairwise_neg_sisdr\n",
    "from asteroid.losses import pairwise_neg_sdsdr\n",
    "from asteroid.losses import pairwise_neg_snr\n",
    "from asteroid.losses import pairwise_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################\n",
    "##### ARGS ##########\n",
    "#####################\n",
    "with open(str(\"cfg.yaml\"), 'r') as file:\n",
    "    CFG = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "DATA_DIR = Path(\"musdb_data\")\n",
    "SEGMENT_SIZE = CFG[\"segment_size\"]\n",
    "RANDOM_TRACK_MIX = CFG[\"random_track_mix\"]\n",
    "TARGETS = CFG[\"targets\"]\n",
    "N_SRC = len(TARGETS)\n",
    "LOSS = eval(CFG[\"loss\"])\n",
    "STORE_GRADIENT_NORM = CFG[\"store_gradient_norm\"]\n",
    "\n",
    "#####################\n",
    "##### HYPER-PARAMETERS\n",
    "#####################\n",
    "SAMPLE_RATE = CFG[\"sample_rate\"]\n",
    "SIZE = None if CFG[\"size\"] == -1 else CFG[\"size\"]\n",
    "LR = CFG[\"learning_rate\"]\n",
    "N_EPOCHS = CFG[\"n_epochs\"]\n",
    "BATCH_SIZE = CFG[\"batch_size\"]\n",
    "\n",
    "N_BLOCKS = CFG[\"n_blocks\"]\n",
    "N_REPEATS = CFG[\"n_repeats\"]\n",
    "BN_CHAN = CFG[\"bn_chan\"]\n",
    "HID_CHAN = CFG[\"hid_chan\"]\n",
    "SKIP_CHAN = CFG[\"skip_chan\"]\n",
    "CONV_KERNEL_SIZE = CFG[\"conv_kernel_size\"]\n",
    "KERNEL_SIZE = CFG[\"kernel_size\"]\n",
    "N_FILTERS = CFG[\"n_filters\"]\n",
    "STRIDE = CFG[\"stride\"]\n",
    "\n",
    "CKP_PATH = Path(\"ckpdir\")\n",
    "if not CKP_PATH.is_dir():\n",
    "    CKP_PATH.mkdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "##### DATA #####\n",
    "################\n",
    "train_dataset = MUSDB18Dataset(\n",
    "    root=DATA_DIR.__str__(),\n",
    "    targets=TARGETS,\n",
    "    suffix=\".mp4\",\n",
    "    split=\"train\",\n",
    "    subset=None,\n",
    "    segment=SEGMENT_SIZE,\n",
    "    samples_per_track=1,\n",
    "    random_segments=True,\n",
    "    random_track_mix=RANDOM_TRACK_MIX,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    size=SIZE\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE)\n",
    "print(\">>> Training Dataloader ready\")\n",
    "\n",
    "test_dataset = MUSDB18Dataset(\n",
    "    root=DATA_DIR.__str__(),\n",
    "    targets=TARGETS,\n",
    "    suffix=\".mp4\",\n",
    "    split=\"test\",\n",
    "    subset=None,\n",
    "    segment=SEGMENT_SIZE,\n",
    "    samples_per_track=1,\n",
    "    random_segments=True,\n",
    "    random_track_mix=RANDOM_TRACK_MIX,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    size=SIZE\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "print(\">>> TEST Dataloader ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "##### MODEL ####\n",
    "################\n",
    "model = ConvTasNet(\n",
    "    n_src=N_SRC,\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_blocks=N_BLOCKS,\n",
    "    n_repeats=N_REPEATS,\n",
    "    bn_chan=BN_CHAN,\n",
    "    hid_chan=HID_CHAN,\n",
    "    skip_chan=SKIP_CHAN,\n",
    "    conv_kernel_size=CONV_KERNEL_SIZE,\n",
    "    norm_type=\"gLN\",\n",
    "    mask_act=\"sigmoid\",\n",
    "    kernel_size=KERNEL_SIZE,\n",
    "    n_filters=N_FILTERS,\n",
    "    stride=STRIDE\n",
    ")\n",
    "\n",
    "loss = PITLossWrapper(LOSS, pit_from=\"pw_mtx\")\n",
    "optimizer = optim.Adam(model.parameters(), lr=LR)\n",
    "lr_updater = lr_scheduler.StepLR(optimizer, 20, 1e-2)\n",
    "history = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "### TRAINING ###\n",
    "################\n",
    "def train(model, dataset, criterion, optimizer, mse):\n",
    "    model.train()\n",
    "    torch.set_grad_enabled(True)\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_mse_loss = 0\n",
    "    data_counter = 0\n",
    "    \n",
    "    for n_batch, train_batch in enumerate(dataset):\n",
    "        x, y = train_batch\n",
    "        \n",
    "        output = model(x)\n",
    "\n",
    "        loss = criterion(output, y)\n",
    "        epoch_mse_loss += mse(output, y).item()\n",
    "        epoch_loss += loss.item()\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        data_counter += batch_size\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if STORE_GRADIENT_NORM:\n",
    "            with open(\"train.log\", \"a\") as log:\n",
    "                for layer in model.modules():\n",
    "                    try:\n",
    "                        name = layer.__str__()\n",
    "                        mean_grad = np.mean(layer.weight.grad.detach().numpy())\n",
    "                        print(\">>> \",name, \" grad =\", mean_grad)\n",
    "                        log.write(f\"NAME : {name}\\nLOSS : {loss.item()}\\nGRADIENT VALUES MEAN: {mean_grad}\\n\\n\")\n",
    "                    except:\n",
    "                        pass\n",
    "        optimizer.step()\n",
    "        \n",
    "    epoch_loss /= data_counter\n",
    "    epoch_mse_loss /= data_counter\n",
    "    \n",
    "    return epoch_loss, epoch_mse_loss\n",
    "\n",
    "\n",
    "def test(model, dataset, criterion, mse):\n",
    "    with torch.no_grad():\n",
    "        mean_loss = 0\n",
    "        mean_mse_loss = 0\n",
    "        data_counter = 0\n",
    "        \n",
    "        for n_batch, test_batch in enumerate(dataset):\n",
    "            x, y = test_batch\n",
    "            \n",
    "            output = model(x)\n",
    "\n",
    "            loss = criterion(output, y)\n",
    "            mean_mse_loss += mse(output, y).item()\n",
    "            mean_loss += loss.item()\n",
    "            \n",
    "            batch_size = x.shape[0]\n",
    "            data_counter += batch_size\n",
    "\n",
    "            optimizer.step()\n",
    "            \n",
    "        mean_loss /= data_counter\n",
    "        mean_mse_loss /= data_counter\n",
    "    \n",
    "    return mean_loss, mean_mse_loss\n",
    "\n",
    "\n",
    "def checkpoint(model, epoch, optimizer, lr_scheduler, best_loss, loss, ckp_dir, delta=1e-3):\n",
    "    if loss <= best_loss + delta:\n",
    "        torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'loss': loss,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_scheduler.state_dict()\n",
    "            },\n",
    "            ckp_dir\n",
    "        )\n",
    "\n",
    "\n",
    "def fit(model, train_set, test_set, criterion, optimizer, lr_updater, epochs, history=None):\n",
    "    \n",
    "    if history is not None:\n",
    "        # Train from checkpoint:\n",
    "        \n",
    "        train_loss_history = list(history[\"train_loss\"].values)\n",
    "        val_loss_history = list(history[\"val_loss\"].values)\n",
    "        train_mse_loss_history = list(history[\"train_mse_loss\"].values)\n",
    "        val_mse_loss_history = list(history[\"val_mse_loss\"].values)\n",
    "        lr_history = list(history[\"lr_history\"].values)\n",
    "        \n",
    "        start_epoch = len(train_loss_history)\n",
    "        print(f\"\\n>>> Restore training from EPOCH {start_epoch}\\n\")\n",
    "    \n",
    "    else:\n",
    "        # Train from scratch:\n",
    "        train_loss_history = list()\n",
    "        val_loss_history = list()\n",
    "        train_mse_loss_history = list()\n",
    "        val_mse_loss_history = list()\n",
    "        lr_history = list()\n",
    "        \n",
    "        start_epoch = 1\n",
    "        print(\"\\n>>> Begin training from scratch\\n\")\n",
    "    \n",
    "    mse = PITLossWrapper(pairwise_mse, pit_from=\"pw_mtx\")\n",
    "    \n",
    "    for epoch in range(start_epoch, start_epoch + epochs + 1):\n",
    "        print(\">>> EPOCH\", epoch)\n",
    "        \n",
    "        train_loss, train_mse_loss = train(model, train_set, criterion, optimizer, mse)\n",
    "        lr_updater.step()\n",
    "        \n",
    "        val_loss, val_mse_loss = test(model, test_set, criterion, mse)\n",
    "        \n",
    "        train_loss_history.append(train_loss)\n",
    "        train_mse_loss_history.append(train_mse_loss)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_mse_loss_history.append(val_mse_loss)\n",
    "        \n",
    "        best_loss = float('inf') if len(val_loss_history) == 0 else np.min(val_loss_history)\n",
    "        \n",
    "        # Save checkpoint:\n",
    "        checkpoint(model, epoch, optimizer, lr_updater, best_loss, val_loss, CKP_PATH/\"model.pth\")\n",
    "        \n",
    "        # Save weights every 10 epochs:\n",
    "        if epoch % 10 == 0:\n",
    "            ckp_dir = str(CKP_PATH/f\"model_epoch{epoch}.pth\")\n",
    "            torch.save(\n",
    "            {\n",
    "                'epoch': epoch,\n",
    "                'loss': val_loss,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'lr_scheduler': lr_updater.state_dict()\n",
    "            },\n",
    "            ckp_dir\n",
    "        )\n",
    "\n",
    "        # Store the learning curves\n",
    "        history = pd.DataFrame(\n",
    "                {\n",
    "                    \"train_loss\": train_loss_history,\n",
    "                    \"val_loss\": val_loss_history,\n",
    "                    \"train_mse_loss\": train_mse_loss,\n",
    "                    \"val_mse_loss\": val_mse_loss\n",
    "                }\n",
    "            )\n",
    "        history.index.name = \"epoch\"\n",
    "        history.to_csv(CKP_PATH/\"HISTORY.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit(model, train_loader, test_loader, loss, optimizer, lr_updater, N_EPOCHS, history)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8ee98f8cc5c61c7bb85862890f9fa26c929bedb663baf44f10e1854389fd85ca"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
