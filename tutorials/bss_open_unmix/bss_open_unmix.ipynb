{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aim of this notebook :\n",
    "The aim of this notebook is to estimate :\n",
    "* Vocals\n",
    "* Bass\n",
    "* Drums\n",
    "* Other (rest of the accompaniment)\n",
    "of the song you want using pretrained models from SigSep Open-Unmix.\n",
    "\n",
    "### Steps :\n",
    "1. Load the song with librosa\n",
    "2. Apply model\n",
    "3. Save and listen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import stempeg\n",
    "import librosa\n",
    "import soundfile\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from openunmix import predict\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.io.wavfile import write\n",
    "from scipy.signal import stft, istft\n",
    "from IPython.display import Audio, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SONGS_DIR = Path(\"songs\")\n",
    "SONG_DIR = SONGS_DIR/\"WishYouWereHere\"\n",
    "path = SONG_DIR/\"9-05 Wish You Were Here.mp3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utils functions\n",
    "\n",
    "def save_estimates(data):\n",
    "    filename = SONG_DIR/'estimates.pkl'\n",
    "    outfile = open(str(filename),'wb')\n",
    "    pickle.dump(estimates, outfile)\n",
    "    outfile.close()\n",
    "\n",
    "def load_estimates(path):\n",
    "    infile = open(path,'rb')\n",
    "    data = pickle.load(infile)\n",
    "    infile.close()\n",
    "    return data\n",
    "\n",
    "def save_audio_files(estimates, rate):\n",
    "    for target, estimate in estimates.items():\n",
    "        track = estimate.detach().cpu().numpy()[0].transpose(1, 0)\n",
    "        write(f\"{SONG_DIR/target}.wav\", rate, track)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "song, rate = librosa.load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x = song[(1*60) * rate : (2*60) * rate]  # Crop the song (inference time consideration)\n",
    "x = song\n",
    "x = np.array([x, x])  # Reshape to stereo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimates = predict.separate(\n",
    "    torch.tensor(x).float(),\n",
    "    rate=rate,\n",
    "    device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_estimates(estimates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_audio_files(estimates, rate*2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Open Unmix model (INRIA) [PyTorch docs](https://pytorch.org/hub/sigsep_open-unmix-pytorch_umx/) :\n",
    "\n",
    "![Diagram](sigsep_umx-diagram.png)\n",
    "\n",
    "The SigSep repo provides a separation pipeline : the **Separator meta-model**\n",
    "1. Compute the mixture's spectrogram using STFT : [Torchaudio transforms](https://pytorch.org/audio/stable/transforms.html)\n",
    "2. Apply multiple spectrogram models (one for each desired target) [model implementation on Github](https://github.com/sigsep/open-unmix-pytorch/blob/master/openunmix/model.py)\n",
    "3. Combine their outputs through a multichannel generalized Wiener filter (using [Norbert](https://github.com/sigsep/norbert))\n",
    "4. Apply the inverse STFT using torchaudio."
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c5c796cb9bc549597c7b2a035dc35f8b5e1d0792b01e265f94d2c19181f45107"
  },
  "kernelspec": {
   "display_name": "Python 3.8.0 64-bit ('.env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
